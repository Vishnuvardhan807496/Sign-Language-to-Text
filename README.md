## Sign Language To Text
Small description about the project like one below
This project bridges the communication gap for deaf and hard-of-hearing people. It translates sign language into text in real-time using a camera and machine learning. Imagine a system that turns hand signs into words on a screen - that's the goal!

## About
<!--Detailed Description about the project-->
Sign language recognition has been widely used for communication amongst the hearing-impaired and non-verbal community. From early electric signal-based sign language recognition to modern-day recognition using machine/deep learning techniques, researchers all over the world have tried to automate this task. This project mainly aim to carry out key point detection based sign language recognition (SLR). Used different machine learning algorithm like random forest, support vector machine and k-nearest neighbour to train the model. 

## Features
<!--List the features of the project as shown below-->
- Implements advance neural network method.
- A framework based application for deployment purpose.
- High scalability.
- Less time complexity.
- A specific scope of Chatbot response model, using json data format.

## Requirements
<!--List the requirements of the project as shown below-->
* Operating System: Requires a 64-bit OS (Windows 10 or Ubuntu) for compatibility with deep learning frameworks.
* Development Environment: Python 3.6 or later is necessary for coding the sign language detection system.
* Deep Learning Frameworks: TensorFlow for model training, MediaPipe for hand gesture recognition.
* Image Processing Libraries: OpenCV is essential for efficient image processing and real-time hand gesture recognition.
* Version Control: Implementation of Git for collaborative development and effective code management.
* IDE: Use of VSCode as the Integrated Development Environment for coding, debugging, and version control integration.
* Additional Dependencies: Includes scikit-learn, TensorFlow (versions 2.4.1), TensorFlow GPU, OpenCV, and Mediapipe for deep learning tasks.

## System Architecture
<!--Embed the system architecture diagram as shown below-->

![Screenshot 2024-03-29 211339](https://github.com/Vishnuvardhan807496/Sign-Language-to-Text/assets/129361378/354fe665-0cc5-409d-bb30-00f0633428d8)



## Output

<!--Embed the Output picture at respective places as shown below as shown below-->
#### Output1 - Name of the output

![WhatsApp Image 2023-12-01 at 11 27 36_f0ba3bd9](https://github.com/Vishnuvardhan807496/Sign-Language-to-Text/assets/129361378/5ee01448-f2b3-4315-9cc4-0988ad27f70a)


#### Output2 - Name of the output

![WhatsApp Image 2023-12-01 at 11 27 51_a17e9f8b](https://github.com/Vishnuvardhan807496/Sign-Language-to-Text/assets/129361378/52dbf5f0-3ff4-4332-8eb0-c92de2de053b)

Detection Accuracy: 96.7%
Note: These metrics can be customized based on your actual performance evaluations.


## Results and Impact
<!--Give the results and impact as shown below-->
This project aims to create a real-time sign language translation system that helps bridge the communication gap for deaf and hearing-impaired individuals. It utilizes a camera to capture sign language gestures and employs machine learning to recognize them. The system extracts key points from the hand, like fingertip positions, and feeds them into a trained model. This model, having learned the patterns from a sign language dataset, translates the individual signs into written text, eliminating the need for a human interpreter. While the initial focus might be on a specific sign language, future advancements could broaden its reach and incorporate more complex features.

This project serves as a foundation for future developments in assistive technologies and contributes to creating a more inclusive and accessible digital environment.

## Articles published / References
1. Duarte, S. Palaskar, L. Ventura, D. Ghadiyaram, K. DeHaan, F. Metze, J. Torres, and X. Giro-i-Nieto. “How2Sign: A Large-scale Multimodal Dataset for Continuous American Sign Language”. In: Conference on Computer Vision and Pattern Recognition (CVPR). 2021. 
2. H. Brashear, T. Starner, P. Lukowicz, and H. Junker. “Using multiple sensors for mobile sign language recognition”. In: Nov. 2005, pp. 45–52. isbn: 0-7695-2034-0. doi: 10.1109/ISWC.2003.1241392. [
